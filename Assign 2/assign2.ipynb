{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ff38415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38220a8c",
   "metadata": {},
   "source": [
    "### Q 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b759203",
   "metadata": {},
   "source": [
    "### a)Minima of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8d4cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient,init_,learn_rate,n=50,tol=1e-06):\n",
    "    x=init_ #initialization of x i.e. start point\n",
    "    for _ in range(n): \n",
    "        delta=-learn_rate*gradient(x) \n",
    "        if np.all(np.abs(delta)<=tol): # iterate untill convergence (checking value small enough)\n",
    "            break\n",
    "        x+=delta # updating value of x\n",
    "    return round(x*1000)/1000\n",
    "def gradient1(x):\n",
    "    #this function return the value of gradient at different point for function (x^2+3x+4)\n",
    "    return 2*x+3\n",
    "def gradient2(x):\n",
    "    #this function return the value of gradient at different point for function (x^2+3x+4)\n",
    "    return (4*x*x*x-6*x+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90654e2d",
   "metadata": {},
   "source": [
    "learning rate = lr =0.1    -> this represent step size in each iteration is 0.1 <br>\n",
    "no. of iteration = n =50   -> represent no. of iteration to reach the result<br>\n",
    "tolerance =tol =1e-06      ->stop iterating if update in current iteration is less than or equal to tolerance <br>\n",
    "initial values for x is 0 <br>\n",
    "gradient(x) function return values of gradient at particular point x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545adb04",
   "metadata": {},
   "source": [
    "####  i) Minima for $ x^2+3*x+4 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc4597b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima of Given Function is  1.75  at  -1.5\n"
     ]
    }
   ],
   "source": [
    "res=gradient_descent(gradient1,0,0.1) #passing function gradient1 that return gradient \n",
    "value=res*res+3*res+4\n",
    "print(\"Minima of Given Function is \", value, \" at \",res)\n",
    "#initial value/start point is 0\n",
    "#learn rate =lr=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92017b3",
   "metadata": {},
   "source": [
    "#### ii) Minima for $ x^4-3*x^2+2*x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "963f89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minima of Given Function is  -4.848076206064  at  -1.366\n"
     ]
    }
   ],
   "source": [
    "res=gradient_descent(gradient2,0,0.1) #passing function gradient2 that return gradient \n",
    "value=res*res*res*res-3*res*res+2*res\n",
    "print(\"Minima of Given Function is \", value, \" at \",res)\n",
    "#initial value/start point is 0\n",
    "#learn rate =lr=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a230fbd8",
   "metadata": {},
   "source": [
    "### b) Gradient function to calculate gradient for Linear regression $ y=ax+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d3a7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(a,b,x,y,N):\n",
    "    ypred=a*x+b   # The current predicted value of y\n",
    "    Y_a=(-2/N)*np.dot((y-ypred),x) #value of partial derivative w.r.t a\n",
    "    Y_b=(-2/N)*sum(y-ypred) #value of partial derivative w.r.t b\n",
    "    return Y_a,Y_b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078ab84",
   "metadata": {},
   "source": [
    "<center>Loss function L</center>\n",
    "$$ L(a,b) = \\frac{1}{N} \\sum_{i=0}^n (y_i-ypred_i)^2 \\\\ ypred=a*X+b $$\n",
    "$$ L(a,b)= \\frac{1}{N} \\sum_{i=0}^n (y_i-(a*x_i+b))^2$$\n",
    "<center> Taking partial derivative of L w.r.t  a </center>\n",
    "$$ \\frac{\\partial L }{\\partial a } = \\frac{1}{N} \\sum_{i=0}^n 2*(y_i-(a*x_i+b))*(-x_i) $$\n",
    "$$ Y_a= \\frac{-2}{N} \\sum_{i=0}^n x_i(y_i-(a*x_i+b))$$\n",
    "<center> Taking partial derivative of L w.r.t  b </center>\n",
    "$$ \\frac{\\partial L }{\\partial b } = \\frac{1}{N} \\sum_{i=0}^n 2*(y_i-(a*x_i+b)) $$\n",
    "$$ Y_b= \\frac{-2}{N} \\sum_{i=0}^n (y_i-(a*x_i+b))$$\n",
    "<center>Above function takes values of a,b,X,y,N and return Y_a and Y_b</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1a8af",
   "metadata": {},
   "source": [
    "### c) Generate artificial data and finding optimal parameters relating X with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0bfd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating artificial data\n",
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000)+1.5 #array of 10000 values with mean =1.5 , stddev=2.5\n",
    "res=1.5*np.random.randn(10000)  #Generate 10000 residual terms\n",
    "y=2+0.3*X+res #Actual values of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ddec1b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values of parameters (a,b) Relating X with y  0.295 2.023\n",
      "Execution time for GD 0.5133240222930908\n"
     ]
    }
   ],
   "source": [
    "def gd_lr(a,b,X,y,n_iter,lr,tol=pow(10,-6)):\n",
    "    n=float(len(X)) # Number of elements in X\n",
    "    for i in range(n_iter):   \n",
    "        ga,gb= gradient(a,b,X,y,n) # calling gradient function return values Y_a and Y_b for given values\n",
    "        delta_a=(-lr)*ga\n",
    "        delta_b=(-lr)*gb\n",
    "        if np.all(np.abs(delta_a)<=tol):  # Checking if the absolute value is small enough\n",
    "                break\n",
    "        if np.all(np.abs(delta_b)<=tol):  # Checking if the absolute value is small enough\n",
    "                break\n",
    "        #update the current value of a and b\n",
    "        a = a + delta_a\n",
    "        b = b + delta_b\n",
    "    return round(a*1000)/1000,round(b*1000)/1000\n",
    "stime=time.time()\n",
    "a,b=gd_lr(0,0,X,y,1000,0.01) \n",
    "etime=time.time()\n",
    "print(\"Optimal values of parameters (a,b) Relating X with y \",a,b)\n",
    "print(\"Execution time for GD\",etime-stime)\n",
    "etime_for_GD=etime-stime #execution time for Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96814b37",
   "metadata": {},
   "source": [
    "The function gd_lr takes parameter a,b,X,y,n_iter,lr,tol .<br>\n",
    "a,b are the parameter that initialize with value 0.<br>\n",
    "X artificial data and Y actual values.<br>\n",
    "tolerance =tol =1e-06 ->stop iterating if update in current iteration is less than or equal to tolerance <br>\n",
    "gradient(a,b,X,y,n) function return values of gradients for current values of a,b. <br>\n",
    "gd_lr() function iterate n_iter no. of times in each iteration it calculate gradient w.r.t a and b values .In each iteration it takes small step of size 0.01 and also update values of a,b. If absolute value is less than or equal to tol=1e-06 then it stop iterating and return values a,b that relate X and y ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b98a7",
   "metadata": {},
   "source": [
    "### d) Minibatch  stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c41721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatchgd(X,y,lr,n,bs,tol=pow(10,-6)):\n",
    "    a=0 #initializing a and b with value 0\n",
    "    b=0\n",
    "    tsample=X.shape[0] #size of dataset\n",
    "    if(bs>tsample): # if batch size greater than no. of points then consider batch size= no. of points\n",
    "        bs=tsample\n",
    "    for i in range(n):\n",
    "        ind=np.random.randint(len(X)-1,size=bs) #randomly selecting bs no. of index \n",
    "        xind=X[ind] \n",
    "        yind=y[ind] \n",
    "        ga,gb=gradient(a,b,xind,yind,bs)#finding gradient for each minibatch for given values\n",
    "        ga=-lr*ga\n",
    "        gb=-lr*gb\n",
    "        if np.all(np.abs(ga)<=tol):  # Checking if the absolute value is small enough\n",
    "            break\n",
    "        if np.all(np.abs(gb)<=tol):  # Checking if the absolute value is small enough\n",
    "            break\n",
    "        # Updating the weight of a and b\n",
    "        b+=gb\n",
    "        a+=ga\n",
    "    return round(a*1000)/1000,round(b*1000)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a28410c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values of parameters (a,b) using minibatch SGD with batch size 500  (0.302, 2.027)\n",
      "Execution time for minibatch 0.1417374610900879\n"
     ]
    }
   ],
   "source": [
    "stime=time.time()\n",
    "print(\"Optimal values of parameters (a,b) using minibatch SGD with batch size 500 \",minibatchgd(X,y,0.01,1000,500) )\n",
    "etime=time.time()\n",
    "print(\"Execution time for minibatch\",etime-stime)\n",
    "etime_for_minibatch=etime-stime #execution time for minibatch Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5cb915",
   "metadata": {},
   "source": [
    "batch size =500 <br>\n",
    "learning rate/step size =0.01 <br>\n",
    "no. of iteration =1000 <br>\n",
    "An implementation of minibatch  Gradient Descent algorithm that computes gradient values using groups of data point in training data, update weight values in the opposite direction of the gradient of the objective function.. In each iteration select bs(i.e. batchsize) no. of indices randomly and find gradient value for that . Using gradient and learning rate update a and b . And return appro.values of a,b that relate X,y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc3cf4",
   "metadata": {},
   "source": [
    "#### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f956db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal values of (a,b) using SGD (0.342, 2.185)\n",
      "Execution time for  SGD  0.05827927589416504\n",
      "Execution time for  minibatch GD  0.1417374610900879\n",
      "Execution time for  GD  0.5133240222930908\n"
     ]
    }
   ],
   "source": [
    "##if batchsize=1 then uses as SGD\n",
    "stime=time.time()\n",
    "print(\"Optimal values of (a,b) using SGD\",minibatchgd(X,y,0.01,1000,1) )\n",
    "etime=time.time()\n",
    "print(\"Execution time for  SGD \",etime-stime) #execution time for stochastic Gradient descent\n",
    "print(\"Execution time for  minibatch GD \",etime_for_minibatch)\n",
    "print(\"Execution time for  GD \",etime_for_GD) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669d944b",
   "metadata": {},
   "source": [
    "SGD is nothing but minibatch with batch size equal to 1 .<br>\n",
    "In SGD ,consider gradient of single data point used it to update a and b. <br>\n",
    "According above execution times SGD takes less time than minibatch and gradient descent , using same learn rate 0.01 and 1000 no. of iteration <br>\n",
    "GD > minibatch > SGD [i.e. GD takes more than minibatch and minibatch takes more time than SGD] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2009ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal minibatch size\n",
    "bs=0 \n",
    "minerror=float(\"inf\")\n",
    "for i in range(50,10000,50): # iterating batch size 50,100,150....\n",
    "    a,b=minibatchgd(X,y,0.01,1000,i) #calcuating for each batch size \n",
    "    ypred=a*X+b\n",
    "    er=np.mean(np.square(y-ypred))#average error between predicted and actual values for given batch size\n",
    "    if(minerror>er): #update minimum error if error for given batchsize less \n",
    "        minerror=er\n",
    "        bs=i #update batchsize for minerror\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "973846f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal batch size for minibatch gradient descent is  5450\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal batch size for minibatch gradient descent is \",bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d73620",
   "metadata": {},
   "source": [
    "### Q2 \n",
    "#### i) the probability that someone has both cold and a fever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325bd47",
   "metadata": {},
   "source": [
    "$$ P(cold=T,fever=T) = P(fever=T|cold=T) * P(cold=T)  $$\n",
    "$$ P(cold=T,fever=T)=0.307∗0.02 $$\n",
    "$$ P(cold=T,fever=T)=0.00614 $$\n",
    "<center> Hence The probability that someone has both cold and a fever is 0.00614"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e6739",
   "metadata": {},
   "source": [
    "#### ii) the probability that someone who has a cough has a cold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989ba16",
   "metadata": {},
   "source": [
    "$$ P(cold=T |cough=T) =[ P(cough=T,cold=T) / P(cough=T) ]     ..............1 $$\n",
    "\n",
    "$\\hspace{1cm}$\n",
    "$$ P(cough = T,cold=T) = P(cough=T|cold=T,lungdisease=T)*P(cold=T)*[P(lungdisease=T|smokes=T)*P(smokes=T)+P(lungdisease=T|smokes=F)*P(smokes=F)]+P(cough=T|cold=T,lungdisease=F)*P(cold=T)*[P(lungdisease=F|smokes=T)*P(smokes=T)+P(lungdisease=F|smokes=F)*P(smokes=F)] $$\n",
    "$\\hspace{1cm}$\n",
    "\n",
    "$$ P(cough = T,cold=T)=0.7525∗0.02[0.1009∗0.2+0.001∗0.8]+0.505∗0.02[0.8991∗0.2+0.999∗0.8] $$\n",
    "\n",
    "$\\hspace{1cm}$\n",
    "<center> Probability someone having cold and cough P(cough = T,cold=T) = 0.010204 \n",
    "\n",
    "$\\hspace{1cm}$\n",
    "$$ P(cough =T) = P(cough=T |cold=F,lungdisease=T)*P(cold=F)*[P(lungdisease=T|smokes=T)*P(smokes=T)+P(lungdisease=T|smokes=F)*P(smokes=F)]+ P(cough=T|cold=T,lungdisease=T)*P(cold=T) * [P(lungdisease=T|smokes=T)*P(smokes=T)+P(lungdisease=T|smokes=F)*P(smokes=F)]+P(cough=T|cold=F,lungdisease=F)*P(cold=F) * [P(lungdisease=F|smokes=T)*P(smokes=T)+P(lungdisease=F|smokes=F)*P(smokes=F)]+P(cough=T|cold=T,lungdisease=F)*P(cold=T) * [P(lungdisease=F|smokes=T)*P(smokes=T)+P(lungdisease=F|smokes=F)*P(smokes=F)] $$\n",
    "\n",
    "$\\hspace{1cm}$\n",
    "$$ P(cough =T) = 0.7525∗0.02[0.1009∗0.2+0.001∗0.8]+0.505∗0.02[0.8991∗0.2+0.999∗0.8]+0.505∗0.98[0.1009∗0.2+0.001∗0.8]+0.01[0.8991∗0.2+0.999∗0.8] $$\n",
    "\n",
    "$\\hspace{1cm}$\n",
    "<center> Probability someone having cough P(cough =T)=0.0302 \n",
    "\n",
    "$\\hspace{1cm}$\n",
    "<center> Substituting values of  P(cough =T,cold=T)  and  P(cough =T)  in equation (1)\n",
    "\n",
    "$\\hspace{1cm}$\n",
    "$$ P(cough = T | cold=T)=(0.0102040  / 0.0302) $$\n",
    "$\\hspace{1cm}$\n",
    "$$ P(cough = T | cold=T)=0.3378 $$\n",
    "$\\hspace{1cm}$\n",
    "<center> The probability that someone who has a cough has a cold is  0.3378 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895f22c",
   "metadata": {},
   "source": [
    "### Q3) MLE for the parameters of a k-sided multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb38d0a",
   "metadata": {},
   "source": [
    "If an observation is  \n",
    "\n",
    "Random variables Xi indicate the number of times outcome number i is observed over the n trials.(X1, ..., Xk) follows a multinomial distribution . suppose k possible mutually exclusive outcomes with  probabilities p1, ..., pk, and n independent trials .\n",
    "$$ n= \\sum_{i=1}^{k} x_i$$\n",
    "\n",
    "Probabilty corresponding to each outcome is\n",
    "$$\\begin{align}\n",
    "p_1 = P(X_1) = \\frac{x_1}{n} \\\\\n",
    "...\\\\\n",
    "p_k = P(X_k) = \\frac{x_k}{n}\n",
    "\\end{align}$$\n",
    "$$\\sum_{i=1}^k p_i = 1$$\n",
    "\n",
    "$${{n}\\choose{x_1, ..., x_k}}=\\frac{n!}{x_1!....x_k!}$$\n",
    "the likelihood for multinomial distribution\n",
    "$$\\begin{align}\n",
    "L(\\mathbf{p}) = {{n}\\choose{x_1, ..., x_k}}\\prod_{i=1}^k p_i^{x_i} \\\\\n",
    "= n! \\prod_{i=1}^k \\frac{p_i^{x_i}}{x_i!}\n",
    "\\end{align}$$\n",
    "and the log-likelihood is\n",
    "$$\\begin{align}\n",
    "l(\\mathbf{p}) = \\log L(\\mathbf{p}) \n",
    "= \\log \\bigg( n! \\prod_{i=1}^k \\frac{p_i^{x_i}}{x_i!} \\bigg)\\\\\n",
    "= \\log n! + \\log \\prod_{i=1}^k \\frac{p_i^{x_i}}{x_i!} \\\\\n",
    "= \\log n! + \\sum_{i=1}^k \\log \\frac{p_i^{x_i}}{x_i!} \\\\\n",
    "= \\log n! + \\sum_{i=1}^k x_i \\log p_i - \\sum_{i=1}^k \\log x_i!\n",
    "\\end{align}$$\n",
    "Using some constraint ($\\sum_{i=1}^k p_i = 1$) with Lagrange multiplier\n",
    "$$\\begin{align}\n",
    "l'(\\mathbf{p},\\lambda) = l(\\mathbf{p}) + \\lambda\\bigg(1 - \\sum_{i=1}^k p_i\\bigg)\n",
    "\\end{align}$$\n",
    "To find $\\arg\\max_\\mathbf{p} L(\\mathbf{p},\\lambda) $\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial p_i} l'(\\mathbf{p},\\lambda) \n",
    "= \\frac{\\partial}{\\partial p_i} l(\\mathbf{p})\n",
    "+ \\frac{\\partial}{\\partial p_i} \\lambda\\bigg(1 - \\sum_{i=1}^k p_i\\bigg) = 0\\\\\n",
    " \\frac{\\partial}{\\partial p_i} \\sum_{i=1}^k x_i \\log p_i\n",
    "- \\lambda \\frac{\\partial}{\\partial p_i} \\sum_{i=1}^k p_i = 0 \\\\\n",
    "\\frac{x_i}{p_i}- \\lambda  = 0 \\\\\n",
    "p_i = \\frac{x_i}{\\lambda} \\\\\n",
    "\\end{align}$$\n",
    " \n",
    "Hence\n",
    "$$\\begin{align}\n",
    "p_i = \\frac{x_i}{\\lambda} \\\\\n",
    "\\sum_{i=1}^k p_i = \\sum_{i=1}^k \\frac{x_i}{\\lambda} \\\\\n",
    "1 = \\frac{1}{\\lambda} \\sum_{i=1}^k x_i \\\\\n",
    "\\lambda = n\n",
    "\\end{align}$$\n",
    "   Thus,\n",
    "$$\\begin{align}\n",
    "p_i = \\frac{x_i}{n}\n",
    "\\end{align}$$\n",
    "The MLE for parameter of k-folded multinomial distribution that maximizes the likelihood of observing the data\n",
    "$$\\begin{align}\n",
    "\\mathbf{p} = \\bigg(\n",
    "\\frac{x_1}{n},\n",
    "...,\n",
    "\\frac{x_k}{n}\n",
    "\\bigg)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85bafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37055b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a3926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
